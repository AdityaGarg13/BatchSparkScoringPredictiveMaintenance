{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Building\n\nUsing the labeled feature data set constructed in the `Code/2_feature_engineering.ipynb` Jupyter notebook, this notebook loads the data from the Azure Blob container and splits it into a training and test data set. We then build a machine learning model (a decision tree classifier or a random forest classifier) to predict when different components within our machine population will fail. We store the better performing model for deployment in an Azure web service in the next. We will prepare and build the web service in the `Code/4_operationalization.ipynb` Jupyter notebook.\n\n**Note:** This notebook will take about 2-4 minutes to execute all cells, depending on the compute configuration you have setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the libraries\n",
    "import time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# for creating pipelines and model\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "\n",
    "# For some data handling\n",
    "\n",
    "# Time the notebook execution. \n",
    "# This will only make sense if you \"Run all cells\"\n",
    "tic = time.time()\n",
    "\n",
    "# This is the final feature data file.\n",
    "TRAINING_TABLE = 'training_data'\n",
    "TESTING_TABLE = 'testing_data'\n",
    "model_type = 'RandomForest' # Use 'DecisionTree' or 'GBTClassifier' or 'RandomForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"TRAINING_TABLE\",TRAINING_TABLE)\n",
    "dbutils.widgets.text(\"model\", model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and dump a short summary of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>\n   machineID        dt_truncated  volt_rollingmean_12  rotate_rollingmean_12  \\\n0         45 2015-10-30 00:00:00           174.611847             450.720590   \n1         45 2015-10-29 12:00:00           166.425099             441.652415   \n2         45 2015-10-29 00:00:00           169.575066             435.441998   \n3         45 2015-10-28 12:00:00           169.190066             448.272390   \n4         45 2015-10-28 00:00:00           172.485177             466.131104   \n5         45 2015-10-27 12:00:00           167.217351             442.999995   \n6         45 2015-10-27 00:00:00           167.309299             432.430837   \n7         45 2015-10-26 12:00:00           172.232218             445.012891   \n8         45 2015-10-26 00:00:00           168.922431             456.224824   \n9         45 2015-10-25 12:00:00           173.080955             455.142128   \n\n   pressure_rollingmean_12  vibration_rollingmean_12  volt_rollingmean_24  \\\n0               102.291328                 40.325912           169.913411   \n1                99.466725                 39.203922           168.000082   \n2                99.452056                 41.994982           169.382566   \n3               101.831761                 40.411471           170.837621   \n4               103.078662                 41.311734           169.851264   \n5                97.828752                 39.624587           167.263325   \n6               102.413577                 40.168805           169.770759   \n7                96.921228                 39.730115           170.577325   \n8                99.373114                 39.491180           171.001693   \n9                99.371852                 39.216454           173.736938   \n\n   rotate_rollingmean_24  pressure_rollingmean_24  vibration_rollingmean_24  \\\n0             447.822531               100.186140                 39.411553   \n1             438.547206                99.459390                 40.599452   \n2             441.857194               100.641908                 41.203226   \n3             457.201747               102.455211                 40.861602   \n4             454.565550               100.453707                 40.468160   \n5             437.715416               100.121164                 39.896696   \n6             438.721864                99.667402                 39.949460   \n7             450.618858                98.147171                 39.610648   \n8             455.683476                99.372483                 39.353817   \n9             454.410161               105.057001                 39.066713   \n\n    ...     error5sum_rollingmean_24  comp1sum  comp2sum  comp3sum  comp4sum  \\\n0   ...                          0.0     411.0     396.0     321.0     516.0   \n1   ...                          0.0     410.0     395.0     320.0     515.0   \n2   ...                          0.0     410.0     395.0     320.0     515.0   \n3   ...                          0.0     409.0     394.0     319.0     514.0   \n4   ...                          0.0     409.0     394.0     319.0     514.0   \n5   ...                          0.0     408.0     393.0     318.0     513.0   \n6   ...                          0.0     408.0     393.0     318.0     513.0   \n7   ...                          0.0     407.0     392.0     317.0     512.0   \n8   ...                          0.0     407.0     392.0     317.0     512.0   \n9   ...                          0.0     406.0     391.0     316.0     511.0   \n\n    model  age    model_encoded  failure  label_e  \n0  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n1  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n2  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n3  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n4  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n5  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n6  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n7  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n8  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n9  model1   14  (0.0, 0.0, 0.0)      0.0      0.0  \n\n[10 rows x 40 columns]\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = spark.sql(\"SELECT * FROM \" + dbutils.widgets.get(\"TRAINING_TABLE\"))\n",
    "train_data.limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Training/Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental practice in machine learning is to calibrate and test your model parameters on data that has not been used to train the model. Evaluation of the model requires splitting the available data into a training portion, a calibration portion and an evaluation portion. Typically, 80% of data is used to train the model and 10% each to calibrate any parameter selection and evaluate your model.\n\nIn general random splitting can be used, but since time series data have an inherent correlation between observations. For predictive maintenance problems, a time-dependent spliting strategy is often a better approach to estimate performance. For a time-dependent split, a single point in time is chosen, the model is trained on examples up to that point in time, and validated on the examples after that point. This simulates training on current data and score data collected in the future data after the splitting point is not known. However, care must be taken on labels near the split point. In this case, feature records within 7 days of the split point can not be labeled as a failure, since that is unobserved data. \n\nIn the following code blocks, we split the data at a single point to train and evaluate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>\n[&apos;volt_rollingmean_12&apos;,\n &apos;rotate_rollingmean_12&apos;,\n &apos;pressure_rollingmean_12&apos;,\n &apos;vibration_rollingmean_12&apos;,\n &apos;volt_rollingmean_24&apos;,\n &apos;rotate_rollingmean_24&apos;,\n &apos;pressure_rollingmean_24&apos;,\n &apos;vibration_rollingmean_24&apos;,\n &apos;volt_rollingmean_36&apos;,\n &apos;vibration_rollingmean_36&apos;,\n &apos;rotate_rollingmean_36&apos;,\n &apos;pressure_rollingmean_36&apos;,\n &apos;volt_rollingstd_12&apos;,\n &apos;rotate_rollingstd_12&apos;,\n &apos;pressure_rollingstd_12&apos;,\n &apos;vibration_rollingstd_12&apos;,\n &apos;volt_rollingstd_24&apos;,\n &apos;rotate_rollingstd_24&apos;,\n &apos;pressure_rollingstd_24&apos;,\n &apos;vibration_rollingstd_24&apos;,\n &apos;volt_rollingstd_36&apos;,\n &apos;rotate_rollingstd_36&apos;,\n &apos;pressure_rollingstd_36&apos;,\n &apos;vibration_rollingstd_36&apos;,\n &apos;error1sum_rollingmean_24&apos;,\n &apos;error2sum_rollingmean_24&apos;,\n &apos;error3sum_rollingmean_24&apos;,\n &apos;error4sum_rollingmean_24&apos;,\n &apos;error5sum_rollingmean_24&apos;,\n &apos;comp1sum&apos;,\n &apos;comp2sum&apos;,\n &apos;comp3sum&apos;,\n &apos;comp4sum&apos;,\n &apos;age&apos;]\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define list of input columns for downstream modeling\n",
    "\n",
    "# We'll use the known label, and key variables.\n",
    "label_var = ['label_e']\n",
    "key_cols =['machineID','dt_truncated']\n",
    "\n",
    "# Then get the remaing feature names from the data\n",
    "input_features = train_data.columns\n",
    "\n",
    "# We'll use the known label, key variables and \n",
    "# a few extra columns we won't need.\n",
    "remove_names = label_var + key_cols + ['failure','model_encoded','model' ]\n",
    "\n",
    "# Remove the extra names if that are in the input_features list\n",
    "input_features = [x for x in input_features if x not in set(remove_names)]\n",
    "\n",
    "input_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark models require a vectorized data frame. We transform the dataset here and then split the data into a training and test set. We use this split data to train the model on 9 months of data (training data), and evaluate on the remaining 3 months (test data) going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">604307\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assemble features\n",
    "va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "train_data = va.transform(train_data).select('machineID','dt_truncated','label_e','features')\n",
    "\n",
    "# set maxCategories so features with > 10 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                               outputCol=\"indexedFeatures\", \n",
    "                               maxCategories=10).fit(train_data)\n",
    "\n",
    "# fit on whole dataset to include all labels in index\n",
    "labelIndexer = StringIndexer(inputCol=\"label_e\", outputCol=\"indexedLabel\").fit(train_data)\n",
    "\n",
    "training = train_data\n",
    "\n",
    "print(training.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models\n\nA particualar troubling behavior in predictive maintenance is machine failures are usually rare occurrences compared to normal operation. This is fortunate for the business as maintenance and saftey issues are few, but causes an imbalance in the label distribution. This imbalance leads to poor performance as algorithms tend to classify majority class examples at the expense of minority class, since the total misclassification error is much improved when majority class is labeled correctly. This causes low recall or precision rates, although accuracy can be high. It becomes a larger problem when the cost of false alarms is very high. To help with this problem, sampling techniques such as oversampling of the minority examples can be used. These methods are not covered in this notebook. Because of this, it is also important to look at evaluation metrics other than accuracy alone.\n\nWe will build and compare two different classification model approaches:\n\n - **Decision Tree Classifier**: Decision trees and their ensembles are popular methods for the machine learning tasks of classification and regression. Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions.\n\n - **Random Forest Classifier**: A random forest is an ensemble of decision trees. Random forests combine many decision trees in order to reduce the risk of overfitting. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks.\n\nWe will to compare these models in the AML Workbench _runs_ screen. The next code block creates the model. You can choose between a _DecisionTree_ or _RandomForest_ by setting the 'model_type' variable. We have also included a series of model hyperparameters to guide your exploration of the model space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = dbutils.widgets.get(\"model\")\n",
    "\n",
    "# train a model.\n",
    "if model_type == 'DecisionTree':\n",
    "  model = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\",\n",
    "                                 # Maximum depth of the tree. (>= 0) \n",
    "                                 # E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'\n",
    "                                 maxDepth=15,\n",
    "                                 # Max number of bins for discretizing continuous features. \n",
    "                                 # Must be >=2 and >= number of categories for any categorical feature.\n",
    "                                 maxBins=32, \n",
    "                                 # Minimum number of instances each child must have after split. \n",
    "                                 # If a split causes the left or right child to have fewer than \n",
    "                                 # minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.\n",
    "                                 minInstancesPerNode=1, \n",
    "                                 # Minimum information gain for a split to be considered at a tree node.\n",
    "                                 minInfoGain=0.0, \n",
    "                                 # Criterion used for information gain calculation (case-insensitive). \n",
    "                                 # Supported options: entropy, gini')\n",
    "                                 impurity=\"gini\")\n",
    "\n",
    "  ##=======================================================================================================================\n",
    "  ## GBTClassifer is only valid for Binary Classifiers, this is a multiclass (failures 1-4) so no GBTClassifier\n",
    "#elif model_type == 'GBTClassifier':\n",
    "#  model = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\",\n",
    "#                        maxIter=200, stepSize=0.1,\n",
    "#                        maxDepth=15,\n",
    "#                        maxBins=32, \n",
    "#                        minInstancesPerNode=1, \n",
    "#                        minInfoGain=0.0)\n",
    "  ##=======================================================================================================================\n",
    "else:\n",
    "  model = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", \n",
    "                                      # Passed to DecisionTreeClassifier\n",
    "                                      maxDepth=15, \n",
    "                                      maxBins=32, \n",
    "                                      minInstancesPerNode=1, \n",
    "                                      minInfoGain=0.0,\n",
    "                                      impurity=\"gini\",\n",
    "                                      # Number of trees to train (>= 1)\n",
    "                                      numTrees=200, \n",
    "                                      # The number of features to consider for splits at each tree node. \n",
    "                                      # Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].\n",
    "                                      featureSubsetStrategy=\"sqrt\", \n",
    "                                      # Fraction of the training data used for learning each  \n",
    "                                      # decision tree, in range (0, 1].' \n",
    "                                      subsamplingRate = 0.632)\n",
    "\n",
    "# chain indexers and model in a Pipeline\n",
    "pipeline_cls_mthd = Pipeline(stages=[labelIndexer, featureIndexer, model])\n",
    "\n",
    "# train model.  This also runs the indexers.\n",
    "model_pipeline = pipeline_cls_mthd.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the model\n\nWe'll save the latest model for use in deploying a webservice for operationalization in the next notebook. We store this local to the Jupyter notebook kernel because the model is stored in a hierarchical format that does not translate to Azure Blob storage well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Model saved\nFull run took 6.11 minutes\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "model_pipeline.write().overwrite().save(\"dbfs:/storage/models/\" + model_type + \".pqt\")\n",
    "print(\"Model saved\")\n",
    "\n",
    "# Time the notebook execution. \n",
    "# This will only make sense if you \"Run All\" cells\n",
    "toc = time.time()\n",
    "print(\"Full run took %.2f minutes\" % ((toc - tic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 0\ndrwxr-xr-x 1 root root 0 Nov 21 17:23 RandomForest.pqt\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sh \n",
    "#mkdir /dbfs/FileStore/models\n",
    "ls -l /dbfs/storage/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n\nIn the next notebook `Code\\4_operationalization.ipynb` Jupyter notebook we will create the functions needed to operationalize and deploy any model to get realtime predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PredictiveMaintenance dlvmjme",
   "language": "python",
   "name": "predictivemaintenance_dlvmjme"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.5.2",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "name": "3_model_building",
  "notebookId": 1.688287220131054E15
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
